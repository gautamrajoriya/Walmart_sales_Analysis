# -*- coding: utf-8 -*-
"""Model (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zrYqKeDND1TWg0Gw4KTt4he5Wllwg3bp
"""

#Importing the Data Manipulation Libraries
import numpy as np
import pandas as pd
#Importing the Data Visualization Libraries
import matplotlib.pyplot as plt
import seaborn as sns
#Importing Data Filter Warning Libraries
import warnings
warnings.filterwarnings('ignore')
#Importing Logging Library
import logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    filemode='w',
                    filename='model.log',force=True)

#Importing Sci-kit Libraries
from sklearn.preprocessing import MinMaxScaler,RobustScaler,StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

url = 'https://raw.githubusercontent.com/gautamrajoriya/Walmart_sales_Analysis/refs/heads/main/WALMART_SALES_ANALYSIS.CSV.csv'
df = pd.read_csv(url)
df.sample(frac=0.5) #Shuffling 50% of the dataset

#Problem Definition
'''
1. To predict whether the person is eligble to approve a loan or not
2. To Understand the business model and provide a suitable soolution for the same.
'''
df.info()

#Checking the null values stats using graphical method
df.isna().sum().plot(kind='barh')

# Checking Missing Value in percentage for every  column present in dataset
missing_values = df.isnull().sum()/ len(df) * 100
missing_values.sort_values(ascending=False)

# Fix column names
df.columns = df.columns.str.strip()

# Fill missing values
df['CPI'] = df['CPI'].fillna(df['CPI'].median())
df['Holiday_Flag'] = df['Holiday_Flag'].fillna(df['Holiday_Flag'].median())

# 1. Remove commas from 'Weekly_Sales' and convert it to numbers
df['Weekly_Sales'] = df['Weekly_Sales'].str.replace(',', '')  # remove commas
df['Weekly_Sales'] = df['Weekly_Sales'].astype(float)          # convert to float

# 2. Convert 'Date' column to date format
# This allows us to do date-based analysis later if needed
df['Date'] = pd.to_datetime(df['Date'])

df['Holiday_Flag'] = df['Holiday_Flag'].astype('category')
df = pd.get_dummies(df, drop_first=True)

# üß™ Features & Target
X = df.drop(columns=['Date', 'Weekly_Sales'])
y = df['Weekly_Sales']

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# ‚öôÔ∏è Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ‚úÇÔ∏è Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# üå≤ Train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# üìä Evaluate
from math import sqrt

y_pred = model.predict(X_test)
rmse = sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R¬≤ Score: {r2:.4f}")

# üìà Feature Importance
importances = model.feature_importances_
features = X.columns
plt.figure(figsize=(8, 5))
sns.barplot(x=importances, y=features)
plt.title("Feature Importances")
plt.show()